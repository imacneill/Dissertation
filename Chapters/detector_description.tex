\chapter{Detector Description}
	\section{Obligatory mention of LHC and various experiments}
	\section{Description of CMS detector}        
		(focus on parts and descriptions that help with e and mu id, isolation measurements, b-tagging, and jet measurements to motivate the reconstruction description of these later on)
	\section{Luminosity and triggering}       
	When operating at full design specification, bunch crossings (and thus particle collisions) are occurring at a rate of 40 GHz (for a crossing every 25 nanoseconds). This rate is far too high for electronics to read out data from every bunch crossing and would create an impossibly large amount of data to store and use. The rate collision rate must be high to because events that contain useful standard model processes such as boson (especially multi-boson production) and top production are rare, and ``new physics'' production (such as SUSY production) is even rare still, if it even exists at all. This allows for a scheme of throwing away the large percentage of data early on that would be rejected upstream anyway by the physicists who use it for research.\\
	
	The scheme for filtering collisions is known as ``triggering.'' CMS is unique amongst detectors in recent history in that it uses a 2 level scheme, while others such as at Zues and the Tevatron used a 3 level scheme. Even ATLAS, the other multipurpose detector at the LHC, also uses a 3 level system (http://cms.web.cern.ch/news/triggering-and-data-acquisition). At the time ATLAS was first being designed, and at previous experiences, telecom switches did not exist that could carry the full 100 kHz output of 1 mb events directly to a computer farm. The 3 level system had a first level implemented in hardware to choose events in the detector (L1) and then a 2nd also implemented in hardware (L2) to further reduce the rate before transfer away from the detector to the computer farms for software triggering (L3) which could implement more advanced decisions.\\
	
	CMS began design shortly after ATLAS and benefitted from more knowledge to predict future hardware trends. The design called for 2 trigger levels: one at hardware level (L1), and one at software level (L2). This has the benefit of allowing for less restrictive triggers that can make more complicated decisions without losing potentially useful events through too crude of a filter.\\
	
	Any downsides?\\
	
	Triggering is a very successful way to reduce the rate of data from the detector, but does have a downside. Although the 2 level system allows for a great deal of flexibility, the hardware triggers are set before runtime. The software triggers are also not changed frequently for a consistent set of decisions to determine the events that are kept. Thus for periods of runs, the triggers are largely static. This creates a difference between algorithms used to reconstruct quantities (such as counting energy in a cone around a particle) at trigger level (``online'' decisions) and similar algorithms used to reconstruct the same quantities at analysis time (``offline''). Care must be taken to require the online algorithms to be a complete subset of the offline algorithms to prevent unintended shaping of the data. Thus if a trigger requires an electron with a maximum amount of energy in a certain sized cone around the electron, this requirement must also be enforced by the physicists analyzing the data. Triggering also produces a very small amount of uncertainty that must be measured and included in the final tabulation of uncertainty on any measurement.\\
	