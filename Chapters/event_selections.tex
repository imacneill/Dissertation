\chapter{Event Selections}
\label{ch:EventSelections}
As explained in Sections~\ref{ch:intro},~\ref{ch:LHC}, and~\ref{ch:particle_reconstruction}, the CMS experiment is a general purpose experiment designed to measure trajectory, momentum, energy, and charge of particles produced in a collision. The collision rate and particle production rate is at an unusably high level for one to look at every event. This is fine because only some events will be interesting. The triggering methods reduces this rate to a level that can be written to storage media. Then these events are processed and put into a dataset. Still, many more events must be removed by a physicist from the dataset in order to only save events germane to a particular measurement. These events are filtered with a ``cut" to remove events that do not match the desired signature (for example in a di-lepton search, events without 2 leptons or more are rejected). Further cuts are applied to ensure a particular quality of the reconstructed objects (for example a cut on the impact parameter of a lepton to help remove fake leptons) to remove false positives. Further selections are also applied to reduce background events that may appear like the signal events. The stringency of the cuts must be carefully balanced to remove as much of the background as possible while maintaining as much of the signal as possible. The following sections describe the cuts and selections to identify \ttZ events and measure their cross section.\\

\section{Triggers for Data Acquisition}
\label{sec:Triggers}
As the desired final state contains three isolated leptons (electrons or muons), a set of triggers without pre-scaling may be used that has one, two, or three isolated leptons as the trigger requirements. The ideal trigger would have moderate \pt thresholds and would not have restrictions on $\eta$ so that leptons from anywhere in the tracker may be counted. Isolation requirements are allowed provided they are not tighter than the offline requirements of 0.09 for electrons (Sec~\ref{sec:ElectronSelections}) and 0.1 for muons (Sec~\ref{sec:MuonSelections}). Further a parallel set of single lepton triggers must exist in order to select data to measure the ratio used in the background subtraction method which estimates non-prompt lepton contamination (i.e. Fake Rate) in the signal (see Sec~\ref{sec:fake_estimation}). \\

Thus a variety of triggers were chosen that require two or more leptons (double lepton triggers). Triggers that require one or more lepton or that require three or more leptons were rejected as they would not match the triggers used to estimate the Fake Rate. The double lepton triggers are assumed to be 100\% efficient. The efficiency to trigger on one lepton is very high, and thus with three leptons, the odds that at least two of them will pass is approximately 100\%. A small systematic uncertainty is added to account for any potential inefficiency (see Sec~\ref{sec:trigger_syst}).
	
\begin{table}[h]
\begin{center}
\caption{\small\label{tab:AnalysisTriggers} Triggers used while measuring the yields in data for the main analysis selections. In the trigger name the ``v" at the end of the line stands for a number of versions that change while data is being collected.}
\begin{tabular}{l|l} \hline \hline
Lepton Type & Trigger  \\ \hline
$\mu\mu$     & \verb=HLT_Mu17_Mu8_v= \\
$\mu \mu$    &\verb=HLT_Mu17_TkMu8_v= \\
ee                   & \verb=HLT_Ele17_CaloIdT_CaloIsoVL_TrkIdVL_TrkIsoVL_v= \\
ee                   & \verb=Ele8_CaloIdT_CaloIsoVL_TrkIdVL_TrkIsoVL_v=\\
$\mu$e          & \verb=HLT_Mu17_Ele8_CaloIdT_CaloIsoVL_TrkIdVL_TrkIsoVL_v= \\
$\mu$e          & \verb=HLT_Mu8_Ele17_CaloIdT_CaloIsoVL_TrkIdVL_TrkIsoVL_v= \\
\hline
\end{tabular}
\end{center}
\end{table}

Events are required to fire a trigger in Data only. Monte Carlo events are not required to fire a trigger. Monte Carlo triggers are not well enough simulated to accurately predict the behavior of Data triggers. This is not a problem as the Data triggers used are un-prescaled and highly efficient. The previously mentioned systematic error on the trigger efficiency in data will cover any differences that may arise in this assumption.




\section{General Event Cleanup and Vertex Selection}
\label{sec::EventCleanup}

Events in data and simulation are required to pass the following:

\begin{itemize}
\item Scraping cut: if there are $\geq$ 10 tracks, require at least 25\% of them to be high purity. 
\item Require at least one good primary vertex (PV), and use the first such vertex found as a reference point for further selections.  A good vertex is selected by requiring:
	\begin{itemize}
%	\item not fake, ?????????
	\item ndof $>$ 4,
	\item $|\rho| < 2$ cm,
	\item $|z| < 24$ cm.  
	\end{itemize}
\end{itemize}
	 
	 
\section{High Level Event Identification}	 
\label{sec:EventSelections}
Good, isolated electrons are selected following the ``medium" identification requirements recommended by the Egamma POG~\cite{egammaidtwiki}.
Electrons are required to have no missing expected inner hits~\cite{conv} and to not be reconstructed as part of a good conversion vertex~\cite{hwwsmurf} so as to suppress background from converted photons.
Additionally, electrons are required to have $\Delta R >0.1$ with respect to any muon passing the selections above.
We reject electrons found in the transition region.
The isolation follows the POG recommendation~\cite{egammapfisotwiki}, using particle-flow based isolation with a cone size of \DR\  $=$ 0.3.  
Subtraction for PU is performed by removing a term defined by the product of the average event energy density and the effective area of the isolation cone from the neutral isolation sum~\cite{egammaisorhoaeff}.
The isolation relative to the electron \pt\ is required to be less than 0.09.\\

Event selections were designed to be middle ground, a compromise between strong background rejection and inclusiveness. It is done in five main steps:
\begin{enumerate}
\item Identify a Z candidate consisting of two isolated high \pt \ leptons (\gt 20 \GeV ) with opposite electric charge and of the same flavor that are within a $\pm 10$ \GeV window of the Z invariant mass (\zmass);
\item Identify a third lepton which could be the result of a W decay passing the same identification and isolation requirements as the Z leptons;
\item Reject events with a fourth lepton passing a looser set of selections (identification loosened to the standard EGamma medium working point and Muon tight working point and $\pt > 10 \ \GeV$) to be exclusive with the four Lepton channel;
\item Identify at least four Jets to be consistent with the number from a semi-leptonic $t\bar{t}$ decay;
\item Identify from the Jets at least two that pass the CSV b-Tagging algorithm;
\end{enumerate}	 
	 
\section{Selections for Identifying Electrons}
\label{sec:ElectronSelections}

Electron candidates are RECO GSF electrons with \pt\ \gt\ 20 \GeV\ and \absetaele\ passing the following requirements recommended by the Egamma POG~\cite{egammaidtwiki}:
\begin{itemize}
\item reject electrons in the transition region $1.4442 < \aeta < 1.556$, where $\eta$ is taken from the super-cluster (SC);
\item electrons have to have $\DR >0.1$ with respect to any muon passing the selections in Sec~\ref{sec:MuonSelections};
\item cut-based medium WP as defined by~\cite{egammaidtwiki};
\item transverse impact parameter of the GSF track with respect to the selected PV to be $<200~\micron$;
\item the $z$ coordinate of the GSF track should be within 2~mm with respect to that of the selected PV;
\item Conversion removal by veto of a good reconstructed conversion vertex.  A conversion vertex is considered good if it has no tracker hits towards the beam, has a fit probability above $10^{-6}$, has a displacement of more than 2~cm, and the  CTF track matching to the electron should be a part of the conversion vertex. No requirement is made on the vertex quality flag corresponding to merging and arbitration~\cite{hwwsmurf}.
\end{itemize}

Additionally, the identification contains the following modifications/additions with respect to the above recommendation:

\begin{itemize}
\item the number of missing expected inner hits must be zero~\cite{conv};

\item the $H/E$ is required to be \lt 0.1 (0.075) in the barrel (endcap) to match the requirements in the trigger.
\end{itemize}

The isolation follows the POG recommendation, using particle-flow based isolation with a cone size of \DR  $=$ 0.3~\cite{egammapfisotwiki}.  
In the endcap, an inner veto of \DR $=$ 0.015 (0.08) is imposed for charged hadrons (photons). 
The isolation is corrected for PU by subtracting from the neutral isolation components a term defined by the product of the average event energy density and the effective area of the isolation cone~\cite{egammaisorhoaeff}.  The neutral component after correction is required to be non-negative.  
The isolation relative to the electron \pt\ is required to be less than 0.09.



\section{Selections for Identifying Muons}
\label{sec:MuonSelections}
Muons are required to be well identified and isolated, passing the ``tight" identification requirements recommended by the muon POG~\cite{muICHEP2012twiki}. Additionally veto deposits are required to be consistent with a minimum ionizing particle and the d0 requirement is tightened to reduce non-prompt lepton contamination.  
Isolation follows the POG recommendation, using  particle-flow based isolation with a $\Delta\beta$ correction for PU and a threshold of 0.1.
A smaller cone size of \DR\ $=$ 0.3 is adopted due to the high hadronic activity expected in signal-like events.\\

Muon candidates are RECO muon objects with \pt\ \gt\ 20 \GeV\ and \absetamu\ and passing the ``tight" identification requirements recommended by the muon POG~\cite{muICHEP2012twiki}:
\begin{itemize}
\item is a global muon;
\item is a particle flow muon;
\item $\chi^2$/ndof of global fit $<$ 10;
\item at least six layers with hits in the tracker;
\item the global fit has to include at least one valid hit in the muon subdetectors;
\item there are muon segments in at least two muon stations. Note, this implies that the muon is also an arbitrated tracker muon~\cite{swguidetrackermuonstwiki};
\item at least one pixel hit.
\end{itemize}

Additionally, the identification contains the following modifications/additions with respect to the above recommendation:

\begin{itemize}
\item transverse impact parameter of the silicon (inner) track with respect to the selected PV to be $<200~\micron$;
\item the inner track $z$ should be within 1~mm from the selected PV;
\item ECAL veto deposit \lt\ 4~\GeV\ (veto deposit corresponds to the sum of $E_T$ in the region of the calorimeter
associated with the muon impact);
\item HCAL veto deposit \lt\ 6~\GeV.
\end{itemize}

The isolation follows the POG recommendation, using particle-flow based isolation with a $\Delta\beta$ correction for PU.  
However, a smaller cone size of \DR\ $=$ 0.3 is adopted due to the high hadronic activity expected in signal-like events.  
The isolation is calculated using 
$$
\relIso = [ \Sigma_{\rm ch} + {\rm max}(0, \Sigma_{\rm nh} + \Sigma_{\rm ph} - 0.5 \Delta\beta) ]/\pt,
$$
where $\Sigma_{\rm ch, nh, ph}$ are the sums of \pt\ of the charged hadron, neutral hadron, and photon particle flow candidates, respectively.
Here the charged hadrons are matched to the PV and a 0.5~\GeV\ threshold is applied on neutral hadrons and photons.  
The $\Delta\beta$ correction is determined from the sum \pt\ of charged hadrons not matched to the PV with a threshold of 0.5~\GeV\ in a cone of the same size as the isolation.  
 
The isolation is required to be less than 0.1 of the \pt.


\section{Selecting the Z-boson Lepton Pair and the 3rd Lepton}
\label{sec:3LepSelection}
All three selected leptons must pass the Identification and Isolation requirements as well as \pt and $\eta$ requirements listed in Secs~\ref{sec:ElectronSelections} and~\ref{sec:MuonSelections}. Furthermore, two of the leptons are required to be consistent with a reconstructed Z boson. This includes:
\begin{itemize}
\item opposite charge
\item same flavor
\item invariant mass between 81 and 101 GeV
\item both leptons matched to the same vertex
\end{itemize}
In the case of events where all three leptons are the same flavor and two combinations exist which fit in the invariant mass window, the one which is closest to the measured Z mass of \zmass ~\cite{pdg} is chosen.\\

\section{Vetoing Events with a 4th Lepton}
\label{sec:4thLeptonVeto}
To be accepted as an event, three tight leptons as described in Secs~\ref{sec:MuonSelections} and~\ref{sec:ElectronSelections} must be found. A veto on a fourth lepton passing the unmodified POG recommendations used as the base cuts above is applied. In addition, the fourth lepton has a relaxed \pt \ requirement of 10 \GeV. Finally, the same isolation requirements as above are used.

\section{Selecting Jets}
\label{sec:JetSelection}
There must be at least three particle-flow jets with $\pt > 30$ GeV and $|\eta| < 2.5$ and at least a fourth particle-flow jet with $\pt > 15$ GeV and $|\eta| < 2.5$. The threshold on the fourth jet is lowered as this one tends to be produced softer than the other jets.
Jets are reconstructed with the anti-$k_{T}$ algorithm with parameter R = 0.5.  
Jets in simulation have L1FastJetL2L3 (FastJet-based offset correction followed by L2 and L3 corrections) corrections applied.  
Jets in data additionally have L2L3 residual corrections applied~\cite{jetcorrectionstwiki}. 
Selected jets must pass loose {\tt pfJetId} and be separated by $\Delta R >$ 0.5 from any lepton passing the selections above. Finally, an extra pile up rejection is applied where at least 10\% \ of the energy in the jet must be from within a $dZ < 0.05$ cm of the primary vertex.\\

\section{Tagging Jets As Originating from a b-quark}
\label{sec:bTagSelection}

b-jets are identified by using the Combined Secondary Vertex (CSV) algorithm. b-jet identification algorithms are known as ``taggers" and the objects they identify are known as ``b-tags," whether those objects are true b-jets or light-flavor jets mis-tagged as a b-jet. The CSV tagger uses a variety of variables as input in an MVA including: the invariant mass of the charged constituents of the jets, multiplicity of charged particles from the secondary vertex, as well as the energy, trajectory, and origins of the tracks in the jet. The algorithm is discussed in more detail in Sec~\ref{sec:csv_reconstruction}. Working points are set for various qualities of tagging. the more stringent requirements enhance the purity of the tags (less mis-tags) at the cost of a lower efficiency for identification (true b-jets are not tagged). Looser requirements are more likely to identify true b-jets at the cost of a higher rate of mis-tags. The three working points are set at ``Loose," ``Medium," and ``Tight" which are designed to allow approximately 10\%, 1\%, and 0.1\% mis-tag rates respectively for the various levels~\cite{btagging}.\\

To help find the \ttZ signature, at least two jets passing the above jet selections in \ref{sec:JetSelection} and $\pt > 30$ are required to be tagged using the CSV algorithm. Both must be at least passing the loose threshold, and one must be at least passing the medium threshold.
This tagger identifies jets with discriminant on a range from 0 to 1  larger than  0.244 as Loose b-tagged and 0.679 as Medium b-tagged~\cite{btagICHEP2012twiki}.\\




\section{Optimizing Selections}
\label{sec:Optimization}

While defining the event selections, a number of options were considered. All of the selections were chosen to simultaneously reduce the  background compared to signal and the projected error on the cross section using the \ttZ \ signal Monte Carlo and full analysis background estimates. This includes \Ht \ cuts, \met \ cuts, alternative thresholds and multiplicities for b-tagging, alternative  \pt cuts and multiplicities for jets, and varying sized Z mass windows. In the end, too many backgrounds have real \met \ from a $W \rightarrow \ell \nu$ decay for this to be a distinguishing variable. Although \Ht \ can be used to reduce many of the backgrounds, it ended up performing not quite as well as and being redundant with the (related) pure jet cuts used in this note. Additionally, a single tight or medium  b-tag was found to let in too much background, while using two mediums or a medium and a tight killed too much signal.\\  

Optimizing the Z mass window ends up being less straightforward than the other relevant selections. The Z mass window makes little difference in the error on the cross section and was thus chosen by its expected signficance (Table ~\ref{tab:zmass_errorsig}). In Table~\ref{tab:zmass_errorsig}, the significance continues to increase with an even narrower Z mass window than the chosen $\pm 10$ \GeV \ window, but there is a strong reason to stop at $\pm 10$ \GeV.\\

These predictions are made with the assumption that no new systematic uncertanties are introduced. This assumption does not hold up for a narrow enough Z mass window. By comparing the yields of the \ttZ \ signal MC to a two lepton Z selection in data, the yields do not scale the same. The two lepton Z selection is the same as used to calculate the rate of b-tagged jets in the background estimate of non-top process. Table ~\ref{tab:zmass_massscaling} shows the \ttZ scaling and the two lepton data scaling both with a b-veto applied as well as a two b-tag selection applied. The reference yields are from the $\pm 15$ \GeV \ Z mass window. The two lepton Z data selection is chosen because it is high in statistics and not part of our signal region (yet contains a reconstructed Z boson).\\

As the window gets narrower beyond $\pm 10$ \GeV, the difference in scaling of the yields in Table ~\ref{tab:zmass_massscaling} becomes significant at approximately a difference of 4-5\%. These narrower mass windows would lead to challenges in estimating the systematic uncertainty introduced by the mass window as well as reduce the significance of that mass window. To avoid this extra difficulty, the $\pm 10$ \GeV \ window is chosen.\\

\begin{table}[ht!]
\caption{\small \label{tab:zmass_errorsig} Expected signficance and estimated \% error on the cross section by varying the Z mass window. Significance and error estimates are determined with full analysis selections and background estimates in conjunction with the \ttZ \ signal Monte Carlo.}
\begin{center}
\begin{tabular}{c|ccc}\hline
Mass Window (\GeV)   &  Expected Significance  & \multicolumn{2}{c}{Estimated Error on Cross Section}    \\
                     &                         & +         & -                                           \\
\hline \hline
$\pm 15$             &  2.27                   &  +51\%    &  -45\%                                      \\
$\pm 12.5$           &  2.42                   &  +50\%    &  -43\%                                      \\
$\pm 10$             &  2.44                   &  +50\%    &  -43\%                                      \\
$\pm 7.5$            &  2.57                   &  +49\%    &  -42\%                                      \\
$\pm 5$              &  2.51                   &  +51\%    &  -43\%                                      \\
$\pm 2.5$            &  2.21                   &  +60\%    &  -49\%                                      \\
\hline
\end{tabular}
\end{center}
\end{table}


%% \begin{table}[ht!]
%% \caption{\small \label{tab:zmass_expsig} Expected significance by varying the Z mass window. Expected significance is determined with full analysis selections and background estimates in conjunction with the \ttZ \ signal Monte Carlo.}
%% \begin{center}
%% \begin{tabular}{c|c}\hline
%% Mass Window (\GeV)  & Expected Significance            \\
%% \hline \hline
%% $\pm 15$            &  2.27 \\
%% $\pm 12.5$          &  2.42 \\
%% $\pm 10$            &  2.44 \\
%% $\pm 7.5$           &  2.57 \\
%% $\pm 5$             &  2.51 \\
%% $\pm 2.5$           &  2.21 \\

%% \hline
%% \end{tabular}
%% \end{center}
%% \end{table}


\begin{sidewaystable}[ht!]
\caption{\small \label{tab:zmass_massscaling} Scaling of yields in high statistics Z selection with Z mass window. The ``2 Lepton Data'' columns are important numbers in the calculation of the rate of b-tags and are used here because they are a high statistics selection of a Z boson. Statistical errors on the absolute yields in the \ttZ MC \ are between 5 and 6\%. Statistical errors on the DY MC are negligable while statistical errors on the 2 Lepton Data selection are between 0.5\% and 2\% where the DY MC with b-veto is clustered at the low end.}
\begin{center}
\begin{tabular}{c|ccccc}\hline
Mass Window (\GeV)  & \ttZ \ MC   & 2 Lepton DY MC  & 2 Lepton Data  & 2 Lepton DY MC  & 2 Lepton Data  \\
                    &             & with b-Veto     & with b-Veto    & with 2 b-tags   & with 2 b-tags \\
\hline \hline
$\pm 15$            &  100\%      &  100\%          & 100\%          & 100\%           & 100\%  \\
$\pm 12.5$          &  99.5\%     &  98.6\%         & 98.1\%         & 99.0\%          & 98.1\% \\
$\pm 10$            &  97.8\%     &  96.0\%         & 95.6\%         & 97.1\%          & 95.4\% \\
$\pm 7.5$           &  95.2\%     &  91.9\%         & 90.8\%         & 91.9\%          & 91.4\% \\
$\pm 5$             &  87.9\%     &  83.6\%         & 81.4\%         & 85.5\%          & 82.0\% \\
$\pm 2.5$           &  65.3\%     &  61.5\%         & 56.8\%         & 60.9\%          & 57.5\% \\

\hline
\end{tabular}
\end{center}
\end{sidewaystable}
